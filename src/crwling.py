import os
import time
import re
from datetime import datetime
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
from src.database import db
from urllib.parse import quote

class NewsToNeo4j:
    def __init__(self):
        try:
            self.driver = db.driver
        except Exception as e:
            print(f"âŒ DB ì—°ê²° ì‹¤íŒ¨: {e}")
            self.driver = None
            
        self.log_dir = "crawl_logs"
        if not os.path.exists(self.log_dir):
            os.makedirs(self.log_dir)
            
            
    def save_debug_info(self, page, name):
        """ì—ëŸ¬ ë°œìƒ ì‹œì ì˜ ìŠ¤í¬ë¦°ìƒ·ê³¼ HTML ì†ŒìŠ¤ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        screenshot_path = os.path.join(self.log_dir, f"{name}_{timestamp}.png")
        html_path = os.path.join(self.log_dir, f"{name}_{timestamp}.html")
        
        page.screenshot(path=screenshot_path)
        with open(html_path, "w", encoding="utf-8") as f:
            f.write(page.content())
        print(f"   ğŸ“¸ ë””ë²„ê·¸ ì •ë³´ ì €ì¥ë¨: {screenshot_path}")
        
    def close(self):
        self.driver.close()

    def clean_text(self, text):
        """ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì •ì œ (íŠ¹ìˆ˜ë¬¸ì ë° ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°)"""
        if not text: return ""
        text = re.sub(r'<[^>]*>', '', text) # HTML íƒœê·¸ ì œê±°
        text = text.replace('\n', ' ').replace('\t', ' ').replace('\r', ' ')
        text = text.replace('\u200b', '').strip()
        return text

    def save_to_neo4j(self, data):
        """ê¸°ì‚¬, ì–¸ë¡ ì‚¬, ê·¸ë¦¬ê³  ë³¸ë¬¸(content)ì„ í¬í•¨í•˜ì—¬ ì €ì¥"""
        query = """
        MERGE (a:Article {link: $link})
        SET a.title = $title, 
            a.content = $content, 
            a.published_at = datetime()
        WITH a
        MERGE (p:Publisher {name: $publisher})
        MERGE (a)-[:WRITTEN_BY]->(p)
        RETURN a
        """
        try:
            with self.driver.session() as session:
                result = session.run(query, data)
                summary = result.consume()
                if summary.counters.nodes_created > 0:
                    print(f"      ğŸ  [DB] ìƒˆ ë…¸ë“œ ìƒì„± ì™„ë£Œ")
                elif summary.counters.properties_set > 0:
                    print(f"      ğŸ”„ [DB] ê¸°ì¡´ ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                return True
            
        except Exception as e:
            print(f"   âŒ DB ì €ì¥ ì—ëŸ¬: {e}")
            return False

    def get_article_content(self, page, url):
        """ê¸°ì‚¬ ìƒì„¸ í˜ì´ì§€ì— ì ‘ì†í•˜ì—¬ ë³¸ë¬¸ì„ ì¶”ì¶œ (ì°¸ê³  ì½”ë“œì˜ iframe ë¡œì§ ë°˜ì˜)"""
        try:
            page.goto(url, wait_until="domcontentloaded", timeout=10000)
            # ë„¤ì´ë²„ ë‰´ìŠ¤ë‚˜ ë¸”ë¡œê·¸ëŠ” íŠ¹ì • ì»¨í…Œì´ë„ˆ ì•ˆì— ë³¸ë¬¸ì´ ìˆìŒ
            # ì—¬ëŸ¬ ì„ íƒìë¥¼ ì‹œë„í•˜ì—¬ ë³¸ë¬¸ì„ ì°¾ìŒ
            content_selectors = [
                "#dic_area", "#articleBodyContents", ".se-main-container", "#articleBody"
            ]
            
            for selector in content_selectors:
                element = page.query_selector(selector)
                if element:
                    return self.clean_text(element.inner_text())
            return ""
        except:
            return ""

    def crawl(self, keyword, pages=1):
        total_saved = 0
        
        with sync_playwright() as p:
            # Docker í™˜ê²½ ìµœì í™” ì„¤ì •
            browser = p.chromium.launch(
                headless=True, 
                args=[
                    '--no-sandbox', 
                    '--disable-setuid-sandbox',
                    '--disable-blink-features=AutomationControlled'
                    ]
                )
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
                locale="ko-KR",
                timezone_id="Asia/Seoul"                
            )
            page = context.new_page()

            for p_num in range(pages):
                start = (p_num * 10) + 1
                # url = f"https://search.naver.com/search.naver?where=news&query={quote(keyword)}&start={start}"
                url = f"https://www.google.com/search?q={quote(keyword)}&tbm=nws&start={start}"
                print(f"\nğŸ“¡ [{p_num+1}/{pages}] í˜ì´ì§€ ìš”ì²­ ì¤‘: {url}")
                                
                try:
                    # í˜ì´ì§€ ì´ë™ ë° ì‘ë‹µ í™•ì¸
                    response = page.goto(url, wait_until="domcontentloaded", timeout=10000)
                    
                    if response:
                        print(f"   ğŸ“¥ [ìƒíƒœì½”ë“œ] {response.status}")
                        if response.status == 429:
                            print("   ğŸš« êµ¬ê¸€ë¡œë¶€í„° ì¼ì‹œì  ì°¨ë‹¨(Too Many Requests)ì„ ë‹¹í–ˆìŠµë‹ˆë‹¤. ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                            break
                        if response.status != 200:
                            print(f"   âš ï¸ ì •ìƒì ì¸ ì‘ë‹µì´ ì•„ë‹™ë‹ˆë‹¤. (Status: {response.status})")
                    
                    # ë‰´ìŠ¤ ì˜ì—­ í™•ì¸
                    if page.query_selector("div#search"):
                        print(f"   ğŸ” [ì„±ê³µ] êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ")
                    else:
                        print(f"   âš ï¸ [ê²½ê³ ] ê²€ìƒ‰ ê²°ê³¼ ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        self.save_debug_info(page, f"no_search_result_p{p_num+1}")
                        continue
                    
                    # ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
                    content = page.content()
                    soup = BeautifulSoup(content, 'html.parser')
                    articles = soup.find_all('div', attrs={'data-ved': True})
                    if not articles:
                        # ë§Œì•½ ìœ„ ë°©ë²•ìœ¼ë¡œë„ ì•ˆ ì¡íˆë©´ ë” ë„“ì€ ë²”ìœ„ë¡œ íƒìƒ‰
                        articles = soup.select('div#rso > div')
                    print(f"   ğŸ“¦ [ì¶”ì¶œ] {len(articles)}ê°œì˜ í›„ë³´ ê¸°ì‚¬ ë°œê²¬")
                    
                    for idx, art in enumerate(articles):
                        try:
                            link_tag = art.find('a', href=True)
                            if not link_tag or 'google.com' in link_tag['href']: continue 
                            
                            title_tag = link_tag.find(['div', 'h3'], attrs={'role': 'heading'})
                            if not title_tag:
                                title_tag = link_tag.find(['div', 'span']) # ë” ìœ ì—°í•˜ê²Œ íƒìƒ‰
                                
                            if not title_tag or len(title_tag.get_text().strip()) < 5: continue

                            # ìƒì„¸ í˜ì´ì§€ ë³¸ë¬¸ ìˆ˜ì§‘ì„ í•  ê²ƒì¸ì§€ ì„ íƒ (ì†ë„ vs ë°ì´í„°ì–‘)
                            link = link_tag['href']
                            content = ""
                            
                            # ë³¸ë¬¸ê¹Œì§€ ê¸ê³  ì‹¶ë‹¤ë©´ í™œì„±í™”
                            if True: 
                                detail_page = context.new_page()
                                content = self.get_article_content(detail_page, link)
                                detail_page.close()

                            data = {
                                'title': title_tag.get_text().strip(),
                                'link': link,
                                'publisher': art.find('span').get_text().strip() if art.find('span') else "Google News", # ë³€ìˆ˜ëª… í†µì¼: source -> publisher
                                'content': content
                            }
                            
                            print(f"   ğŸ“ ({idx+1}) ë°ì´í„° ì¶”ì¶œ ì„±ê³µ: {data['title'][:20]}...")

                            if self.save_to_neo4j(data):
                                total_saved += 1
                                
                        except Exception as inner_e:
                            print(f"      â— [íŒŒì‹± ì—ëŸ¬] {inner_e}")
                            continue
                                
                except Exception as e:
                    print(f"   âŒ [í˜ì´ì§€ ì—ëŸ¬] {type(e).__name__}")
                    self.save_debug_info(page, f"page_error_p{p_num+1}")
                    continue
                            
                                
                            

            browser.close()
        print(f"\nâœ¨ ìµœì¢… {total_saved}ê±´ Neo4j ì €ì¥ ì™„ë£Œ.")
