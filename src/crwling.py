import time
import re
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
from src.database import db
from urllib.parse import quote

class NaverNewsToNeo4j:
    def __init__(self):
        self.driver = db.driver

    def close(self):
        self.driver.close()

    def clean_text(self, text):
        """ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì •ì œ (íŠ¹ìˆ˜ë¬¸ì ë° ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°)"""
        if not text: return ""
        text = re.sub(r'<[^>]*>', '', text) # HTML íƒœê·¸ ì œê±°
        text = text.replace('\n', ' ').replace('\t', ' ').replace('\r', ' ')
        text = text.replace('\u200b', '').strip()
        return text

    def save_to_neo4j(self, data):
        """ê¸°ì‚¬, ì–¸ë¡ ì‚¬, ê·¸ë¦¬ê³  ë³¸ë¬¸(content)ì„ í¬í•¨í•˜ì—¬ ì €ì¥"""
        query = """
        MERGE (a:Article {link: $link})
        SET a.title = $title, 
            a.content = $content, 
            a.published_at = datetime()
        WITH a
        MERGE (p:Publisher {name: $publisher})
        MERGE (a)-[:WRITTEN_BY]->(p)
        RETURN a
        """
        try:
            with self.driver.session() as session:
                session.run(query, data)
                return True
        except Exception as e:
            print(f"   âŒ DB ì €ì¥ ì—ëŸ¬: {e}")
            return False

    def get_article_content(self, page, url):
        """ê¸°ì‚¬ ìƒì„¸ í˜ì´ì§€ì— ì ‘ì†í•˜ì—¬ ë³¸ë¬¸ì„ ì¶”ì¶œ (ì°¸ê³  ì½”ë“œì˜ iframe ë¡œì§ ë°˜ì˜)"""
        try:
            page.goto(url, wait_until="domcontentloaded", timeout=10000)
            # ë„¤ì´ë²„ ë‰´ìŠ¤ë‚˜ ë¸”ë¡œê·¸ëŠ” íŠ¹ì • ì»¨í…Œì´ë„ˆ ì•ˆì— ë³¸ë¬¸ì´ ìˆìŒ
            # ì—¬ëŸ¬ ì„ íƒìë¥¼ ì‹œë„í•˜ì—¬ ë³¸ë¬¸ì„ ì°¾ìŒ
            content_selectors = [
                "#dic_area", "#articleBodyContents", ".se-main-container", "#articleBody"
            ]
            
            for selector in content_selectors:
                element = page.query_selector(selector)
                if element:
                    return self.clean_text(element.inner_text())
            return ""
        except:
            return ""

    def crawl(self, keyword, pages=1):
        total_saved = 0
        
        with sync_playwright() as p:
            # Docker í™˜ê²½ì—ì„œëŠ” ë°˜ë“œì‹œ no-sandbox ì˜µì…˜ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ
            browser = p.chromium.launch(headless=True, args=['--no-sandbox', '--disable-setuid-sandbox'])
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            page = context.new_page()

            for p_num in range(pages):
                start = (p_num * 10) + 1
                url = f"https://search.naver.com/search.naver?where=news&query={quote(keyword)}&start={start}"
                
                print(f"ğŸ” í˜ì´ì§€ {p_num + 1} ì ‘ì† ì¤‘...")
                page.goto(url, wait_until="domcontentloaded")
                
                try:
                    page.wait_for_selector(".news_tit", timeout=10000)
                except:
                    print(f"âš ï¸ {p_num + 1}í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨ (ìº¡ì°¨ ê°€ëŠ¥ì„±)")
                    continue

                # ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
                content = page.content()
                soup = BeautifulSoup(content, 'html.parser')
                articles = soup.select('div.news_area')

                for art in articles:
                    title_tag = art.select_one('a.news_tit')
                    press_tag = art.select_one('a.info.press')
                    
                    if not title_tag: continue
                    
                    link = title_tag['href']
                    title = title_tag.get_text(strip=True)
                    publisher = press_tag.get_text(strip=True) if press_tag else "ì•Œìˆ˜ì—†ìŒ"

                    # [í•µì‹¬ ë³´ì™„] ìƒì„¸ í˜ì´ì§€ ë“¤ì–´ê°€ì„œ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
                    detail_page = context.new_page()
                    article_content = self.get_article_content(detail_page, link)
                    detail_page.close()

                    data = {
                        'title': title,
                        'link': link,
                        'publisher': publisher,
                        'content': article_content
                    }
                    
                    if self.save_to_neo4j(data):
                        print(f"   âœ… [ì €ì¥] {title[:20]}...")
                        total_saved += 1
                    
                    time.sleep(1) # ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ ì§§ì€ íœ´ì‹

            browser.close()
        print(f"\nâœ¨ ìµœì¢… {total_saved}ê±´ Neo4j ì €ì¥ ì™„ë£Œ.")
